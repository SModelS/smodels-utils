Artificial proto-modelling
==========================

The goal of this project is to arrive at plausible precursors of the Next Standard
Model, constructed purely from data. Technically, the project consists of four
components a "builder", a "critic", a "combiner", and a "predictor".

-) a proto-model builder: a Monte-Carlo Markov Chain-type algorithm that constructs
proto-models in a random walk, arbitrarily changing the particle content of the theory,
	the masses, cross sections, and branching ratios of the hypothetical new particles.
Once a proto-model is constructed, it is passed on to the critic.

-) the critic takes the proto-model from the builder, and uses SModelS to confront it against
the SModelS database of simplified models results from about 100 searches. The aim 
of the critic is to reject proto-models that are excluded by any of the plethora of results 
in the SModelS database. As a means of robustification, models that are excluded by one result
by allowed by similar, other results, will not be rejected. In case of acceptance of
the model, the critic passes the model to the combiner, alongside with the information
about how close the model has become to an exclusion. In case of an exclusion,
the critic reports back which part of the model was responsible for the exclusion,
leveraging the decomposition procedure of SModelS.

-) the combiner takes the model that passed all tests for exclusion and searches for hints
of a dispersed signal in the data. It does so by finding that "legal" combination of
results that maximally violates the Standard Model hypothesis. In this context, legal 
combinations are defined to be combinations of analyses that can be treated as approximately 
uncorrelated. Also, if a certain combination is considered to be "legal", then any subset 
of the analyses are not -- if a result can be added to a combination, it has to be added.

-) the combiner reports the final statistical score back to the model builder, which 
uses it as its criterion in the random walk.

-) Finally, the predictor runs alongside with the combiner and the critic, and learns
to be predict the verdicts of both the critic and the combiner with a neural network.
Once the network is trained well enough, it can be used to compute the gradient of 
the statistical scores: the model builder can perform gradient ascent on the score in
order to more efficiently find optimal models. An optimal model, to repeat, is a model 
that maximally violates the Standard Model hypothesis while evading all simplified models
constraints.

A first proof-of-principle for the entire setup exists and is running [1]. Within
the scope of this ERC grant, we wish to bring the prototype to full maturity.

-) The PostDoc is envisaged to be working full-time on the statistical,
numerical, and theoretical core aspects of the project. The current prototype
needs to be developed to a production-grade framework. The model builder
needs to be generalized to allow for more types of theoretical models. For
instance, the current implementation of the builder can construct only models
with promptly decaying particles, while a more generalized version will have
to allow also for meta-stable particles whose decay length is at the scale of
the LHC detectors. Such a generalization will also trigger changes in the
other parts of the framework. For example, results from long-lived particle
searches and searches for exotic signatures will have to systematically added to 
the database. The SModelS core software will have to be extended. 
The process will be described in several publications that focus on the
technical and algorithmic aspects of the project.
In a latter stage of the project, the framework will be run at a large computing 
cluster. The resulting best proto-models will be scrutinized in great detail. 
One or more publications are envisaged that discuss the physics implications. 
The proto-models shall also be used to propose new signatures to search for, 
and new search strategies. Such proposals should be an essential part of our 
"physics" publications. Within this grant we would be compelled to work
with experimental groups at implementing our proposals. Any resulting 
CMS or ATLAS publications would however be considered to be outside the scope
of this grant.

-) The PhD student is meant to help the PostDoc with many of the more technical
and time-consuming tasks like augmenting the SModelS database, running the large-scale
random walks, training the "predictor" neural network, and scrutinizing the output.
Depending on the composition of the group, the PhD can specialize either in more algorithmic
aspects of the entire program, or those aspects that require a deeper knowledge of 
(theoretical) physics.

-) In a later stage of the project we wish to bring in information from measurements (as opposed to searches) via likelihoods on Wilson coefficients. 

[1] Insert a link to a recent talk
